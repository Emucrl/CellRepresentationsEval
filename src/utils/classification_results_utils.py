from pathlib import Path
from typing import Dict, List

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import torch
import torchmetrics


def moving_average_by_ratio(
    results: pd.DataFrame,
    metrics: List[str],
) -> pd.DataFrame:
    """Smooth metrics results with moving average window size=1
    over the training_ratio

    Args:
        results (pd.DataFrame): results df generated by the classification pipelines with training_ratio col
        metrics (List[str]):List of metrics to smooth values. Each metric should be a col in results df

    Returns:
        pd.DataFrame: results df with updated metrics values
    """
    ratios = sorted(results.training_ratio.unique())
    results_by_ratio = []
    for index, ratio in enumerate(ratios):
        if index == 0:
            targets = ratios[:2]
        elif index == len(ratios) - 1:
            targets = ratios[-2:]
        else:
            targets = ratios[index - 1 : index + 2]
        df_ratio = results[results["training_ratio"].isin(targets)]
        temp = df_ratio.groupby(["iteration", "exp"])[metrics].mean().reset_index()
        temp.insert(column="training_ratio", loc=0, value=ratio)
        results_by_ratio.append(temp)
    return pd.concat(results_by_ratio, ignore_index=True)


def plot_results(output_file: Path, smooth: bool = True) -> None:
    """Plot metric by training_ration for all metrics
    plots are saved in output_file.parent / plots
    Args:
        output_file (Path): path to results df
        smooth (bool, optional): Use moving average smoothing. Defaults to True.
    """
    (output_file.parent / "plots").mkdir(parents=True, exist_ok=True)
    results = pd.read_parquet(output_file)
    results["exp"] = (
        results["exp"]
        .replace(
            {
                "embeddings": "DGI",
                "supervised gcn": "Supervised GCN",
                "rawfeatures": "Raw features",
                "normalized utag": "UTAG",
            }
        )
        .cat.remove_unused_categories()
    )

    results["training_ratio"] = results["training_ratio"].astype(float)

    results = results.rename(columns={"auc": "AUC", "map": "Average precision"})
    metrics = ["f1", "recall", "precision", "accuracy", "AUC", "Average precision"]
    if smooth:
        results = moving_average_by_ratio(results, metrics)

    results = results.rename(
        columns={"training_ratio": "Ratio of training patients used"}
    )

    for metric in metrics:
        fig, axe = plt.subplots(figsize=(10, 8))

        fig = sns.lineplot(
            data=results,
            x="Ratio of training patients used",
            y=metric,
            hue=results["exp"],
            errorbar=None,
            hue_order=sorted(list(results["exp"].unique())),
            ax=axe,
        )
        plt.legend(loc="lower right")
        fig.figure.savefig(
            output_file.parent / "plots" / f"{metric}_by_ratio_plots.svg",
            bbox_inches="tight",
            transparent="True",
            pad_inches=0,
        )

        plt.close()


def get_binary_classif_results(preds: torch.Tensor, gts: torch.Tensor) -> Dict:
    """Use torchmetrics to compute binary classification metrics

    Args:
        preds (torch.Tensor)
        gts (torch.Tensor)

    Returns:
        Dict: output results, including preds and gts tensors
    """
    preds = preds.cpu()
    gts = gts.cpu()
    results = {
        "accuracy": torchmetrics.Accuracy("binary")(  # pylint: disable=not-callable
            torch.softmax(preds, 1)[:, 1] > 0.5, gts
        ).item(),
        "recall": torchmetrics.Recall("binary")(  # pylint: disable=not-callable
            torch.softmax(preds, 1)[:, 1] > 0.5, gts
        ).item(),
        "precision": torchmetrics.Precision("binary")(  # pylint: disable=not-callable
            torch.softmax(preds, 1)[:, 1] > 0.5, gts
        ).item(),
        "f1": torchmetrics.F1Score("binary")(  # pylint: disable=not-callable
            torch.softmax(preds, 1)[:, 1] > 0.5, gts
        ).item(),
        "auc": torchmetrics.AUROC("binary")(  # pylint: disable=not-callable
            torch.softmax(preds, 1)[:, 1], gts
        ).item(),
        "map": torchmetrics.AveragePrecision("binary")(  # pylint: disable=not-callable
            torch.softmax(preds, 1)[:, 1], gts
        ).item(),
        "probas": np.array(torch.softmax(preds, 1)[:, 1].cpu()).astype(np.float64),
        "labels": np.array(gts.cpu()).astype(np.float64),
    }

    return results


def get_multiclass_classif_results(
    preds: torch.Tensor, gts: torch.Tensor, n_classes: int
) -> Dict:
    """Use torchmetrics to compute binary classification metrics

    Args:
        preds (torch.Tensor)
        gts (torch.Tensor)

    Returns:
        Dict: output results, including preds and gts tensors"""

    preds = preds.cpu()
    gts = gts.cpu()
    results = {
        "accuracy": torchmetrics.Accuracy(  # pylint: disable=not-callable
            "multiclass",
            num_classes=n_classes,
            average="weighted",
        )(preds, gts).item(),
        "recall": torchmetrics.Recall(  # pylint: disable=not-callable
            "multiclass", num_classes=n_classes, average="weighted"
        )(preds, gts).item(),
        "precision": torchmetrics.Precision(  # pylint: disable=not-callable
            "multiclass", num_classes=n_classes, average="weighted"
        )(preds, gts).item(),
        "f1": torchmetrics.F1Score(  # pylint: disable=not-callable
            "multiclass", num_classes=n_classes, average="weighted"
        )(preds, gts).item(),
        "auc": torchmetrics.AUROC(  # pylint: disable=not-callable
            "multiclass", num_classes=n_classes, average="weighted"
        )(preds, gts).item(),
        "map": torchmetrics.AveragePrecision(  # pylint: disable=not-callable
            "multiclass",
            average="weighted",
            num_classes=n_classes,
        )(preds, gts).item(),
        "probas": np.array(preds.cpu()).astype(np.float64),
        "labels": np.array(gts.cpu()).astype(np.float64),
    }

    return results
